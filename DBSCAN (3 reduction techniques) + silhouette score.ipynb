{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"DataSets/CleanedDataset/combined_dataset.csv\")  # Change to your actual filename\n",
    "\n",
    "# Keep only the 'Description' column\n",
    "descriptions = df[\"Description\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"Cleaned_Description\"] = descriptions.apply(preprocess_text)\n",
    "\n",
    "df.to_csv(\"DataSets/DBSCAN_Datasets/cleaned_base_dataset.csv\", index=False)\n",
    "\n",
    "###############################\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"DataSets/DBSCAN_Datasets/cleaned_base_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Cleaned_Description\"])\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "tfidf_matrix_normalized = scaler.fit_transform(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_tech = 'pca'  # Toggle this to change dimensionality reduction technique\n",
    "\n",
    "if reduction_tech.lower() == 'pca':\n",
    "    reducer = PCA(n_components=2, random_state=42)\n",
    "elif reduction_tech.lower() == 'tsne':\n",
    "    reducer = TSNE(n_components=2, random_state=42)\n",
    "else:\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, min_dist=0.1)\n",
    "    #reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "\n",
    "reduced_embeddings = reducer.fit_transform(tfidf_matrix_normalized)\n",
    "\n",
    "df[\"X\"] = reduced_embeddings[:, 0]\n",
    "df[\"Y\"] = reduced_embeddings[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, test, feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df.to_csv(\"DataSets/DBSCAN_Datasets/train_data.csv\", index=False)\n",
    "val_df.to_csv(\"DataSets/DBSCAN_Datasets/val_data.csv\", index=False)\n",
    "\n",
    "print(\"Feature engineering complete using TF-IDF + \", reduction_tech.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use nearest neighbors to fit and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DataSets/DBSCAN_Datasets/train_data.csv\")\n",
    "X = df[[\"X\", \"Y\"]].values\n",
    "\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "\n",
    "distances = np.sort(distances[:, 4], axis=0)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Data Points (sorted)\")\n",
    "plt.ylabel(\"5th Nearest Neighbor Distance\")\n",
    "plt.title(\"K-Distance Graph for Optimal eps\")\n",
    "plt.savefig(\"DBSCAN_graphs/\" + reduction_tech + \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop over distances array to find the best hyperparams to use: eps with the best silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eps = None\n",
    "best_score = -1\n",
    "best_min_samples = 5  # Try reducing this to 3, 5, etc.\n",
    "\n",
    "# Find the maximum values of the distances array\n",
    "max_distance = np.max(distances)\n",
    "\n",
    "# Iterate over different eps values\n",
    "for eps in np.arange(0.1, max_distance, 0.01):  # Adjusted eps range\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=best_min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Skip if no valid clusters are found\n",
    "    if len(set(labels) - {-1}) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Evaluate silhouette score\n",
    "    score = silhouette_score(X, labels)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_eps = eps\n",
    "\n",
    "# Fallback if no best eps is found\n",
    "if best_eps is None:\n",
    "    print(\"No valid eps found! Consider adjusting min_samples or checking data distribution.\")\n",
    "    best_eps = 0.5\n",
    "\n",
    "print(f\"Best eps: {best_eps}, Silhouette Score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply DBSCAN with best eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN with the best eps found\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "df[\"Cluster\"] = dbscan.fit_predict(X)\n",
    "df.to_csv(\"DataSets/DBSCAN_Datasets/dbscan_clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final result (with spectral clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"X\"], df[\"Y\"], c=df[\"Cluster\"], cmap=\"Spectral\", marker=\"o\", edgecolor=\"k\")\n",
    "plt.title(f\"DBSCAN Clustering [{reduction_tech.upper()}] (Silhouette Score: {best_score:.3f})\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.text(0.05, 0.95, f\"eps = {best_eps}\", fontsize=12, ha='left', va='top', transform=plt.gca().transAxes, color='white')\n",
    "plt.text(0.05, 0.90, f\"min_samples = {best_min_samples}\", fontsize=12, ha='left', va='top', transform=plt.gca().transAxes, color='white')\n",
    "plt.savefig(\"DBSCAN_graphs/dbscan_clustering_visualization_{reduction_tech}.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Clustering visualization saved as 'DBSCAN_graphs/dbscan_clustering_visualization_{reduction_tech}.png'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score interpretation:\n",
    "0.6 - 0.7: Fairly good clustering. The points are reasonably well-clustered, and the separation between clusters is decent.\n",
    "\n",
    "0.7 - 1.0: Very good clustering. The points are well-grouped within clusters, and the clusters are clearly defined.\n",
    "\n",
    "0.0 - 0.6: Clustering can be improved. The clusters might be overlapping or not very distinct.\n",
    "Negative values: Points are likely misclassified or the clustering algorithm might not be appropriate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
